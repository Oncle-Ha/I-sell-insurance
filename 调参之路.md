模型



1. 根据PSC和RC关键特征扩展几维，并保留原来的PSC和RC。

   <img src="C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20201026164344239.png" alt="image-20201026164344239" style="zoom:67%;" />

   ​	*Area under curve (AUC):  0.85868*

2. 多项式扩展维度（degree=2）

   *Area under curve (AUC):  0.85896*（不进行特征选择的情况）

   - Area under curve (AUC):  0.85948

     ```python
     sel_model = SelectFromModel(clf, prefit=True, threshold=-np.inf, max_features=30)
     train_X_2 = sel_model.transform(train_X_plus)
     print("新的特征数量:", train_X_2.shape[1])
     ```

   * Area under curve (AUC):  0.85897 
    ```python
   sel_model = SelectFromModel(clf, prefit=True, threshold=-np.inf, max_features=40)
   train_X_2 = sel_model.transform(train_X_plus)
   print("新的特征数量:", train_X_2.shape[1])
    ```

   * 35: 0.85947
   * 33: 
   * 28: 0.85922

   - | max_features | valid   | test    | train   |
     | ------------ | ------- | ------- | ------- |
     | 35           | 0.85947 | 0.8584  | 0.86366 |
     | 33           | 0.85918 | 0.85846 | 0.86342 |
     | 32           | 0.85948 | 0.85861 | 0.86301 |
     | 31           | 0.85962 | 0.85848 | 0.86267 |
     | 30           | 0.85948 |         |         |
     | 29           | 0.85951 | 0.85852 | 0.86105 |

     

   - 考虑到模型数据越多对模型训练效果越好，我们把训练集调整到整个数据集的0.99对模型进行训练，模型性能应该有所提升。

     |      | valid   | test   | train   |
     | ---- | ------- | ------ | ------- |
     | 30   | 0.86005 | 0.8605 | 0.86275 |

   - 依靠什么分类器计算特征的importance
     - GBDT：	以上
     - XGBOOST：0.856（效果没有GBDT好）

3. 对XGBOOST调参

    **Bayesian Optimization with HYPEROPT**.

   ## **初始贝叶斯优化(Bayesian Optimization Primer)**

   网格搜索也随机搜索的问题在于**不是启发式的**调参，先验的模型结果对于后续模型调整的方向毫无帮助。在人工调优时，我们的调参吗方法一般是基于目标函数，正向或反向调整超参数值的过程（比如1，2，3效果一般，7，8，9效果不好，我们大概率参数搜索空间就会落在1，2，3附近），但非启发式的参数搜索就做不到这点。**贝叶斯优化的精髓就在于它根据先前结果的验证情况来挑选下一次迭代的超参数**。这就使得模型能够**花更多时间在相对较好的参数中比较精确的找到最好的那一组参数，而不是漫无目的的在参数空间中浪费我们的计算资源**。





4. 特征聚类
   - 不使用多项式升维
   - 使用多项式升维